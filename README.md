# BIG Data Final Project

### Author: Rajeev Chapagain

### Data Source: https://www.gutenberg.org/ebooks/65129

### [Databrick Link](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4054073891972817/3726290276480636/2632073140528245/latest.html)

### Technologies used:
1. Databrick Cloud Environment
1. Spark Processing Engine
1. pysprak
1. pandas libraries
1. Python

### Process:
-  #### Achieving Data

   We get the data from the url using urllib.request library. Once we get the data we save the data in temporary folder and        name it HowtoStudy.txt
   
   ```
      import urllib.request
      urllib.request.urlretrieve("https://www.gutenberg.org/files/65129/65129-0.txt" , "/tmp/HowtoStudy.txt")
   ```
   
   Now we have to move data into temporary and we do it using dbutils.fs.mv which need two arguments;One is the file which we      want to moce and another is the desired location
   
   ```
   dbutils.fs.mv("file:/tmp/HowtoStudy.txt","dbfs:/data/HowtoStudy.txt")
   ```

   Now , we have to transfer file into Spark which convert data into RDD
   ```
   HTSRawRDD= sc.textFile("dbfs:/data/HowtoStudy.txt")
   ```
   ### Data Cleaning
   - Here, we will be breaking down the data with the help of flatmapping. In this process, the capitalized word will be             converted o lower case, spacces will be removed and the word will be generated by splitting the sentences
    ```
    HTSMessyTokensRDD = HTSRawRDD.flatMap(lambda line: line.lower().strip().split(" "))
    ```
   - Now, the punctuation ill be removed with the help of re library. Anything except letter will be removed
      ```
      import re
      HTSCleanTokensRDD = HTSMessyTokensRDD.map(lambda letter: re.sub(r'[^A-Za-z]', '', letter))
      ```
   - The next step is to remove stopword . The library named StopWordsRemover will be used here to do a job
     ```
     from pyspark.ml.feature import StopWordsRemover
     remover = StopWordsRemover()
     stopwords = remover.getStopWords()
     HTSWordsRDD = HTSCleanTokensRDD.filter(lambda PointLessW: PointLessW not in stopwords)
     
     ```
     
    - In this process we filter out empty spaces
      
      ```
      HTSEmptyRemoveRDD = HTSWordsRDD.filter(lambda x: x != "")
      ```
### Data Processing

  - Now we map our words into intermediate key value pairs.
    ```
    HTSPairsRDD = HTSEmptyRemoveRDD.map(lambda word: (word,1))
    ```
  - Changing pair into word counts with this step, The repeated wor willbe added in the value section of key value pairs:
    ```
    HTSWordCountRDD = HTSPairsRDD.reduceByKey(lambda acc, value: acc + value)
    ```
  - Now, the word will be sorte in descending order with sortByKey. Once they are done will use the top 10items in our list         with the help of take command
    ```
    HTSResults = HTSWordCountRDD.map(lambda x: (x[1], x[0])).sortByKey(False).take(10)
    print(HTSResults)
    
    ```
### Visualization
   
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns

    source = 'How to Study Fiorillo by Edith Lynwood Winn'
    title = 'Top Words in ' + source
    xlabel = 'Count'
    ylabel = 'Words'

    df = pd.DataFrame.from_records(HTSResults, columns =[xlabel, ylabel]) 
    plt.figure(figsize=(15,5))
    sns.barplot(xlabel, ylabel, data=df, palette="rocket").set_title(title)
    
   
### References:
   - [DataBricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4574377819293972/2246755934805346/3186223000943570/latest.html)
   - [SeaBorn](https://seaborn.pydata.org/generated/seaborn.barplot.html)

     
      
  
   
